# Day 1 NLP Deep Dive: Tokenization & Embeddings

## Part 1: Text Preprocessing

Text preprocessing is the foundation of any NLP pipeline. Raw text contains noise, inconsistencies, and variations that can confuse machine learning models. Preprocessing transforms this messy text into a clean, standardized format.

### Why Preprocessing Matters

Imagine feeding a model these three sentences:
- "The cat sat on the mat."
- "THE CAT SAT ON THE MAT!!!"
- "the    cat   sat on the mat"

Without preprocessing, a model might treat these as completely different inputs, even though they convey the same meaning.

### Common Preprocessing Steps

**1. Lowercasing**

Converts all text to lowercase to reduce vocabulary size and treat "Cat", "cat", and "CAT" as the same word.

```python
text = "The Quick Brown Fox JUMPS!"
processed = text.lower()
# Output: "the quick brown fox jumps!"
```

**2. Removing Punctuation & Special Characters**

```python
import re
text = "Hello, World! How's it going? #NLP"
processed = re.sub(r'[^\w\s]', '', text)
# Output: "Hello World Hows it going NLP"
```

**3. Removing Stop Words**

Stop words are common words (the, is, at, which) that often don't carry significant meaning.

```python
from collections import Counter

stop_words = {'the', 'is', 'at', 'which', 'on', 'a', 'an'}
text = "the cat is on the mat"
words = [w for w in text.split() if w not in stop_words]
# Output: ['cat', 'mat']
```

**4. Stemming vs Lemmatization**

- **Stemming**: Chops off word endings (running → run, ran → ran)
- **Lemmatization**: Uses vocabulary and morphological analysis (running → run, ran → run)

```python
# Simple stemming example
def simple_stem(word):
    suffixes = ['ing', 'ed', 's', 'ly']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

words = ['running', 'ran', 'runs', 'quickly']
stemmed = [simple_stem(w) for w in words]
# Output: ['runn', 'ran', 'run', 'quick']
```

**5. Normalization**

Handling unicode, accents, and character variations:

```python
import unicodedata

text = "café résumé naïve"
normalized = unicodedata.normalize('NFKD', text)
ascii_text = normalized.encode('ASCII', 'ignore').decode('ASCII')
# Output: "cafe resume naive"
```

### Complete Preprocessing Pipeline

```python
import re

def preprocess_text(text, remove_stopwords=True):
    # Lowercase
    text = text.lower()
    
    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    
    # Remove mentions and hashtags
    text = re.sub(r'@\w+|#\w+', '', text)
    
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Remove stop words (optional)
    if remove_stopwords:
        stop_words = {'the', 'is', 'at', 'which', 'on', 'a', 'an', 'in', 'to'}
        words = [w for w in text.split() if w not in stop_words]
        text = ' '.join(words)
    
    return text

# Example usage
raw_text = "Check out this awesome NLP tutorial! https://example.com #MachineLearning @AIEnthusiast"
clean_text = preprocess_text(raw_text)
print(clean_text)
# Output: "check out awesome nlp tutorial machinelearning aienthusiast"
```

---

## Part 2: Tokenization - Breaking Text into Pieces

Tokenization is the process of splitting text into smaller units called tokens. These tokens are the atomic units that models process.

### Word-Level Tokenization

The simplest approach: split on whitespace and punctuation.

```python
text = "Hello, world! How are you?"

# Simple whitespace split
tokens = text.split()
# ['Hello,', 'world!', 'How', 'are', 'you?']

# Better approach with regex
import re
tokens = re.findall(r'\b\w+\b', text.lower())
# ['hello', 'world', 'how', 'are', 'you']
```

**Problems with Word-Level Tokenization:**

1. **Huge vocabulary**: English has 170,000+ words
2. **Out-of-vocabulary (OOV) words**: What about "COVID-19", "blockchain", or misspellings?
3. **Language-specific**: Doesn't work well for languages without spaces (Chinese, Japanese)

### Character-Level Tokenization

Split text into individual characters.

```python
text = "hello"
tokens = list(text)
# ['h', 'e', 'l', 'l', 'o']
```

**Advantages**: Small vocabulary (26 letters + symbols)
**Disadvantages**: Very long sequences, loses word-level meaning

### Subword Tokenization: The Best of Both Worlds

Modern LLMs use subword tokenization algorithms that balance vocabulary size and semantic meaning.

---

## Part 3: BPE (Byte Pair Encoding)

BPE is an iterative algorithm that starts with characters and progressively merges the most frequent pairs.

### How BPE Works

**Step 1**: Start with all characters as individual tokens

```
Text: "low low low lower lowest"
Initial tokens: l, o, w, space, l, o, w, e, r, s, t
```

**Step 2**: Find the most frequent pair and merge it

```
Most frequent pair: ('l', 'o') appears 5 times
Merge into: 'lo'
New tokens: lo, w, space, lo, w, e, r, s, t
```

**Step 3**: Repeat until desired vocabulary size

```
Iteration 2: ('lo', 'w') → 'low'
Iteration 3: ('low', 'e') → 'lowe'
Iteration 4: ('lowe', 'r') → 'lower'
...and so on
```

### BPE Implementation

```python
import re
from collections import Counter, defaultdict

class SimpleBPE:
    def __init__(self, num_merges=10):
        self.num_merges = num_merges
        self.merges = {}
        self.vocab = set()
    
    def get_pairs(self, word):
        """Get all adjacent pairs in a word"""
        pairs = []
        prev_char = word[0]
        for char in word[1:]:
            pairs.append((prev_char, char))
            prev_char = char
        return pairs
    
    def train(self, corpus):
        """Train BPE on a corpus"""
        # Split corpus into words
        words = corpus.lower().split()
        
        # Initialize vocabulary with characters
        vocab = defaultdict(int)
        for word in words:
            # Add space delimiter to mark word boundaries
            vocab[' '.join(list(word)) + ' </w>'] += 1
        
        # Perform merges
        for i in range(self.num_merges):
            pairs = defaultdict(int)
            
            # Count all pairs
            for word, freq in vocab.items():
                symbols = word.split()
                for pair in zip(symbols[:-1], symbols[1:]):
                    pairs[pair] += freq
            
            if not pairs:
                break
            
            # Find most frequent pair
            best_pair = max(pairs, key=pairs.get)
            self.merges[best_pair] = ''.join(best_pair)
            
            # Merge the best pair in vocabulary
            new_vocab = {}
            bigram = ' '.join(best_pair)
            replacement = ''.join(best_pair)
            
            for word, freq in vocab.items():
                new_word = word.replace(bigram, replacement)
                new_vocab[new_word] = freq
            
            vocab = new_vocab
            
            print(f"Merge {i+1}: {best_pair} → {replacement}")
        
        # Build final vocabulary
        self.vocab = set()
        for word in vocab.keys():
            self.vocab.update(word.split())
        
        return self.vocab
    
    def tokenize(self, text):
        """Tokenize text using learned merges"""
        words = text.lower().split()
        tokenized = []
        
        for word in words:
            # Start with characters
            word_tokens = list(word) + ['</w>']
            
            # Apply merges
            while len(word_tokens) > 1:
                pairs = [(word_tokens[i], word_tokens[i+1]) 
                        for i in range(len(word_tokens)-1)]
                
                # Find the first pair in our merge rules
                pair_to_merge = None
                for pair in pairs:
                    if pair in self.merges:
                        pair_to_merge = pair
                        break
                
                if pair_to_merge is None:
                    break
                
                # Merge the pair
                first, second = pair_to_merge
                new_tokens = []
                i = 0
                while i < len(word_tokens):
                    if (i < len(word_tokens) - 1 and 
                        word_tokens[i] == first and 
                        word_tokens[i+1] == second):
                        new_tokens.append(first + second)
                        i += 2
                    else:
                        new_tokens.append(word_tokens[i])
                        i += 1
                word_tokens = new_tokens
            
            tokenized.extend(word_tokens)
        
        return tokenized

# Example usage
corpus = "low low low lower lowest new newer newest"
bpe = SimpleBPE(num_merges=10)
vocab = bpe.train(corpus)

print("\nFinal vocabulary:")
print(sorted(vocab))

print("\nTokenizing new text:")
tokens = bpe.tokenize("lower newest")
print(tokens)
```

**Output:**
```
Merge 1: ('l', 'o') → lo
Merge 2: ('lo', 'w') → low
Merge 3: ('e', 'r') → er
Merge 4: ('n', 'e') → ne
Merge 5: ('ne', 'w') → new
...

Final vocabulary:
['</w>', 'e', 'l', 'low', 'lower', 'newest', 'new', 'o', 'r', 's', 't', 'w']

Tokenizing new text:
['lower', '</w>', 'newest', '</w>']
```

### Why BPE is Powerful

1. **Handles rare words**: "unhappiness" → ["un", "happiness"]
2. **Compact vocabulary**: Typically 30,000-50,000 tokens instead of millions
3. **No OOV problem**: Any word can be broken into subwords
4. **Learns meaningful units**: Common prefixes/suffixes emerge naturally

---

## Part 4: WordPiece Tokenization

WordPiece is similar to BPE but uses a different scoring mechanism. Instead of frequency, it chooses merges that maximize the likelihood of the training data.

### How WordPiece Differs from BPE

**BPE**: Merges most frequent pair
**WordPiece**: Merges pair that increases training data likelihood the most

### WordPiece Implementation

```python
import math
from collections import defaultdict

class SimpleWordPiece:
    def __init__(self, vocab_size=100):
        self.vocab_size = vocab_size
        self.vocab = set()
        
    def train(self, corpus, min_freq=2):
        """Train WordPiece tokenizer"""
        words = corpus.lower().split()
        word_counts = Counter(words)
        
        # Start with characters
        self.vocab = set()
        for word in word_counts.keys():
            self.vocab.update(list(word))
        self.vocab.add('[UNK]')  # Unknown token
        self.vocab.add('##')      # Continuation marker
        
        # Iteratively add subwords
        while len(self.vocab) < self.vocab_size:
            # Get all subwords from current vocabulary
            subword_counts = defaultdict(int)
            
            for word, count in word_counts.items():
                # Try to segment word with current vocab
                start = 0
                while start < len(word):
                    end = len(word)
                    found = False
                    
                    while start < end:
                        subword = word[start:end]
                        if start > 0:
                            subword = '##' + subword
                        
                        if subword in self.vocab:
                            subword_counts[subword] += count
                            found = True
                            break
                        end -= 1
                    
                    if not found:
                        # Single character fallback
                        subword = word[start]
                        if start > 0:
                            subword = '##' + subword
                        subword_counts[subword] += count
                        end = start + 1
                    
                    start = end
            
            # Find best new subword by likelihood
            best_score = -float('inf')
            best_subword = None
            
            for word, count in word_counts.items():
                for i in range(len(word)):
                    for j in range(i + 2, len(word) + 1):
                        candidate = word[i:j]
                        if i > 0:
                            candidate = '##' + candidate
                        
                        if candidate not in self.vocab and len(candidate) > 1:
                            # Simple scoring: log probability
                            score = math.log(subword_counts.get(candidate, 0) + 1)
                            if score > best_score:
                                best_score = score
                                best_subword = candidate
            
            if best_subword is None:
                break
            
            self.vocab.add(best_subword)
            print(f"Added: {best_subword} (score: {best_score:.2f})")
        
        return self.vocab
    
    def tokenize(self, word):
        """Tokenize a single word"""
        tokens = []
        start = 0
        
        while start < len(word):
            end = len(word)
            found = False
            
            while start < end:
                substr = word[start:end]
                if start > 0:
                    substr = '##' + substr
                
                if substr in self.vocab:
                    tokens.append(substr)
                    found = True
                    break
                end -= 1
            
            if not found:
                tokens.append('[UNK]')
                break
            
            start = end
        
        return tokens

# Example usage
corpus = "looking looked look seeing saw see running run"
wp = SimpleWordPiece(vocab_size=30)
vocab = wp.train(corpus)

print("\nTokenizing examples:")
for word in ['looking', 'running', 'walked']:
    tokens = wp.tokenize(word)
    print(f"{word} → {tokens}")
```

**Key Insights:**

- `##` prefix marks continuation tokens (not word-start)
- "unhappiness" → ["un", "##happiness"] or ["un", "##happi", "##ness"]
- BERT uses WordPiece with ~30,000 token vocabulary

---

## Part 5: Why Embeddings Work - The Magic of Vector Spaces

Embeddings transform discrete tokens (words, subwords) into continuous vector spaces where semantic relationships become geometric relationships.

### The Core Intuition

**Problem**: Computers can't understand "cat" or "dog" as concepts.

**Solution**: Represent words as vectors (lists of numbers) where similar words have similar vectors.

```
cat    → [0.8, 0.3, -0.2, 0.5]
dog    → [0.75, 0.35, -0.15, 0.48]
car    → [-0.1, 0.9, 0.7, -0.3]
truck  → [-0.15, 0.85, 0.75, -0.25]
```

Notice: cat and dog vectors are similar, car and truck are similar, but cat and car are different.

### Vector Space Properties

In embedding space, we can do arithmetic:

```
king - man + woman ≈ queen
Paris - France + Italy ≈ Rome
```

### How Embeddings Learn Meaning

Embeddings are learned by predicting context. The **distributional hypothesis**: *"Words that occur in similar contexts tend to have similar meanings."*

**Word2Vec Approach** (Simplified):

Given a sentence: "The cat sat on the mat"

- Predict center word from context: [The, sat] → cat
- Or predict context from center word: cat → [The, sat]

Words that can fill the same role (cat, dog, rat) end up with similar vectors because they appear in similar contexts.

### Simple Embedding Implementation

```python
import numpy as np

class SimpleEmbedding:
    def __init__(self, vocab_size, embedding_dim):
        # Random initialization (in practice, these are learned)
        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.1
        self.word_to_idx = {}
        self.idx_to_word = {}
    
    def build_vocab(self, corpus):
        """Build vocabulary from corpus"""
        words = set(corpus.lower().split())
        for idx, word in enumerate(sorted(words)):
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
    
    def get_embedding(self, word):
        """Get embedding vector for a word"""
        idx = self.word_to_idx.get(word, 0)
        return self.embeddings[idx]
    
    def similarity(self, word1, word2):
        """Compute cosine similarity between two words"""
        emb1 = self.get_embedding(word1)
        emb2 = self.get_embedding(word2)
        
        # Cosine similarity
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        return dot_product / (norm1 * norm2)
    
    def most_similar(self, word, top_k=5):
        """Find most similar words"""
        word_emb = self.get_embedding(word)
        similarities = []
        
        for other_word in self.word_to_idx.keys():
            if other_word != word:
                sim = self.similarity(word, other_word)
                similarities.append((other_word, sim))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

# Example usage
corpus = "cat dog animal pet car truck vehicle transport"
emb = SimpleEmbedding(vocab_size=10, embedding_dim=50)
emb.build_vocab(corpus)

# In practice, embeddings would be trained, but let's simulate
# Let's manually set some vectors for demonstration
emb.embeddings[emb.word_to_idx['cat']] = np.array([0.8, 0.3, -0.2] + [0]*47)
emb.embeddings[emb.word_to_idx['dog']] = np.array([0.75, 0.35, -0.15] + [0]*47)
emb.embeddings[emb.word_to_idx['car']] = np.array([-0.1, 0.9, 0.7] + [0]*47)

print(f"Similarity(cat, dog): {emb.similarity('cat', 'dog'):.3f}")
print(f"Similarity(cat, car): {emb.similarity('cat', 'car'):.3f}")
```

### Why Embeddings Work: Mathematical Perspective

Embeddings compress high-dimensional sparse representations (one-hot vectors) into low-dimensional dense representations.

**One-hot encoding (BAD for large vocabularies):**
```
Vocabulary size: 50,000 words
"cat" = [0, 0, 0, ..., 1, ..., 0]  (50,000 dimensions, mostly zeros)
```

**Embedding (GOOD):**
```
"cat" = [0.8, 0.3, -0.2, 0.5, ...]  (300 dimensions, all informative)
```

**Key Properties:**

1. **Dimensionality reduction**: 50,000 → 300 dimensions
2. **Semantic similarity**: Similar meanings → similar vectors
3. **Continuous space**: Enables interpolation and arithmetic
4. **Learned representation**: Captures linguistic patterns from data

### Practical Embedding Training (Skip-gram)

```python
import numpy as np
from collections import defaultdict

class SimpleSkipGram:
    def __init__(self, vocab_size, embedding_dim, learning_rate=0.01):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lr = learning_rate
        
        # Two matrices: input and output embeddings
        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01
        self.W_out = np.random.randn(embedding_dim, vocab_size) * 0.01
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()
    
    def train_pair(self, center_idx, context_idx):
        """Train on one (center, context) pair"""
        # Forward pass
        h = self.W_in[center_idx]  # Hidden layer (embedding)
        u = np.dot(h, self.W_out)  # Output scores
        y_pred = self.softmax(u)   # Predicted probabilities
        
        # True label (one-hot)
        y_true = np.zeros(self.vocab_size)
        y_true[context_idx] = 1
        
        # Backward pass (simplified)
        error = y_pred - y_true
        
        # Update output weights
        self.W_out -= self.lr * np.outer(h, error)
        
        # Update input weights
        dh = np.dot(self.W_out, error)
        self.W_in[center_idx] -= self.lr * dh
        
        # Compute loss
        loss = -np.log(y_pred[context_idx] + 1e-10)
        return loss

# Demonstration
vocab = {'cat': 0, 'dog': 1, 'sat': 2, 'mat': 3}
model = SimpleSkipGram(vocab_size=4, embedding_dim=10)

# Training on "cat sat"
# Center: cat (0), Context: sat (2)
loss = model.train_pair(center_idx=0, context_idx=2)
print(f"Training loss: {loss:.3f}")

# After training, W_in contains our word embeddings
print(f"\nEmbedding for 'cat': {model.W_in[0][:5]}")  # Show first 5 dims
```

---

## Part 6: Putting It All Together - Complete Demo

Here's a complete pipeline from raw text to embeddings:

```python
import numpy as np
import re
from collections import Counter, defaultdict

class NLPPipeline:
    def __init__(self, embedding_dim=50, vocab_size=1000):
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.embeddings = None
        
    def preprocess(self, text):
        """Clean and normalize text"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def build_vocab(self, corpus):
        """Build vocabulary from corpus"""
        words = corpus.split()
        word_counts = Counter(words)
        
        # Take most common words
        most_common = word_counts.most_common(self.vocab_size - 2)
        
        # Special tokens
        self.word_to_idx['<PAD>'] = 0
        self.word_to_idx['<UNK>'] = 1
        
        for idx, (word, _) in enumerate(most_common, start=2):
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        print(f"Vocabulary size: {len(self.word_to_idx)}")
    
    def tokenize(self, text):
        """Convert text to token indices"""
        words = text.split()
        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 = <UNK>
        return indices
    
    def initialize_embeddings(self):
        """Initialize random embeddings"""
        vocab_len = len(self.word_to_idx)
        self.embeddings = np.random.randn(vocab_len, self.embedding_dim) * 0.1
        self.embeddings[0] = 0  # <PAD> is zero vector
    
    def get_sentence_embedding(self, text):
        """Get embedding for entire sentence (average of word embeddings)"""
        indices = self.tokenize(text)
        word_embeds = [self.embeddings[idx] for idx in indices]
        return np.mean(word_embeds, axis=0)
    
    def semantic_similarity(self, text1, text2):
        """Compute similarity between two sentences"""
        emb1 = self.get_sentence_embedding(text1)
        emb2 = self.get_sentence_embedding(text2)
        
        # Cosine similarity
        dot_prod = np.dot(emb1, emb2)
        norm_prod = np.linalg.norm(emb1) * np.linalg.norm(emb2)
        
        return dot_prod / norm_prod

# Complete example
corpus = """
the cat sat on the mat. the dog played in the yard.
cats and dogs are popular pets. many people love animals.
cars and trucks drive on roads. vehicles need fuel to run.
"""

# Initialize pipeline
pipeline = NLPPipeline(embedding_dim=50, vocab_size=100)

# Preprocess
clean_corpus = pipeline.preprocess(corpus)
print("Preprocessed corpus:")
print(clean_corpus[:100] + "...\n")

# Build vocabulary
pipeline.build_vocab(clean_corpus)

# Initialize embeddings
pipeline.initialize_embeddings()

# Tokenize sample text
sample = "the cat sat on the mat"
tokens = pipeline.tokenize(sample)
print(f"Tokenized '{sample}':")
print(f"Token IDs: {tokens}\n")

# Get embeddings
sentence_embedding = pipeline.get_sentence_embedding(sample)
print(f"Sentence embedding (first 10 dims): {sentence_embedding[:10]}\n")

# Compare similarities
sent1 = "cats and dogs are pets"
sent2 = "dogs and cats are animals"
sent3 = "cars and trucks are vehicles"

print(f"Similarity('{sent1}', '{sent2}'): {pipeline.semantic_similarity(sent1, sent2):.3f}")
print(f"Similarity('{sent1}', '{sent3}'): {pipeline.semantic_similarity(sent1, sent3):.3f}")
```

---

## Part 7: Why This Matters for LLMs

Modern LLMs like GPT, BERT, and Claude build on these foundations:

### The LLM Architecture Stack

```
Input Text
    ↓
[Text Preprocessing]
    ↓
[Tokenization (BPE/WordPiece)]
    ↓
[Token → ID mapping]
    ↓
[Embedding Layer] ← This is what we built!
    ↓
[Transformer Layers]
    ↓
[Output Predictions]
```

### Key Insights

1. **Tokenization is critical**: Poor tokenization = poor model performance
   - Too many tokens: expensive, slow
   - Too few tokens: loss of meaning
   
2. **Embeddings capture semantics**: The first layer learns meaningful representations
   - Similar words cluster together
   - Relationships emerge (king-queen, big-small)

3. **Subword tokenization solves real problems**:
   - No more out-of-vocabulary errors
   - Handles rare words, typos, new words
   - Works across languages

### Real-World Impact

**GPT-4**: Uses ~100,000 token vocabulary (BPE)
**BERT**: Uses ~30,000 token vocabulary (WordPiece)
**Claude**: Uses custom tokenization optimized for multi-language support

These choices directly affect:
- Model size
- Training cost
- Inference speed
- Multilingual capabilities

---

## Summary: Day 1 Essentials

### Text Preprocessing
- **Purpose**: Clean and normalize raw text
- **Key techniques**: Lowercasing, punctuation removal, stop word filtering, stemming/lemmatization
- **Why it matters**: Reduces noise, standardizes input, improves model efficiency

### Tokenization Approaches

| Method | Vocabulary Size | Pros | Cons |
|--------|----------------|------|------|
| Word-level | Very large (100K+) | Simple, preserves meaning | OOV problems, huge vocab |
| Character-level | Small (~100) | No OOV, works everywhere | Very long sequences, loses meaning |
| **BPE** | Medium (30-100K) | ✓ No OOV ✓ Compact ✓ Learns morphology | Slightly complex |
| **WordPiece** | Medium (30-100K) | ✓ Likelihood-based ✓ Used in BERT | Similar to BPE |

### BPE Algorithm
1. Start with characters as base vocabulary
2. Count all adjacent pair frequencies
3. Merge most frequent pair
4. Repeat until desired vocabulary size
5. Result: Learns common subwords naturally

### WordPiece Algorithm
- Similar to BPE but uses likelihood scoring
- Chooses merges that maximize training data probability
- Adds `##` prefix for continuation tokens
- Used in BERT and many modern models

### Why Embeddings Work
- **Core idea**: Transform discrete tokens → continuous vectors
- **Distributional hypothesis**: Similar contexts → similar meanings
- **Vector arithmetic**: Enables semantic operations (king - man + woman ≈ queen)
- **Dimensionality**: 50,000 sparse → 300 dense dimensions
- **Training**: Learn by predicting context (Word2Vec, GloVe) or through language modeling

### Embeddings Enable
✓ Semantic similarity measurement
✓ Analogical reasoning
✓ Transfer learning
✓ Continuous optimization
✓ Efficient computation

### Interview-Ready Soundbite

*"Tokenization matters because it determines how efficiently a model can represent language. Modern LLMs use subword tokenization (BPE/WordPiece) to balance vocabulary size with semantic granularity. By breaking words into meaningful subunits, we eliminate out-of-vocabulary problems while keeping computational costs manageable. The learned embeddings then map these tokens into a continuous space where linguistic relationships become geometric, enabling the model to generalize across similar contexts."*

### Practical Takeaways

**For Your Projects:**

1. **Start with existing tokenizers**: Don't build from scratch
   ```python
   # Use HuggingFace tokenizers
   from transformers import AutoTokenizer
   tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
   ```

2. **Preprocessing checklist**:
   - Always lowercase for case-insensitive tasks
   - Keep punctuation for sentiment analysis (! and ? matter)
   - Remove stop words only for search/IR tasks
   - Never remove stop words for language modeling

3. **Tokenization best practices**:
   - Use BPE for multilingual models
   - Use WordPiece for BERT-based models
   - Vocabulary size: 30K-50K is the sweet spot
   - Always handle `<UNK>` tokens gracefully

4. **Embedding tips**:
   - Start with pretrained embeddings (Word2Vec, GloVe, FastText)
   - Fine-tune on your domain data
   - Typical dimensions: 50-300 (more isn't always better)
   - Use cosine similarity for semantic comparisons

**For Interviews:**

Key questions you should be able to answer:
- "Why not just use word-level tokenization?" → OOV problem, huge vocabulary
- "What's the difference between BPE and WordPiece?" → Frequency vs likelihood scoring
- "How do embeddings capture meaning?" → Distributional hypothesis + context prediction
- "Why 300 dimensions for embeddings?" → Balance between expressiveness and computation

**For Your Resume:**

Strong bullet points:
- "Implemented BPE tokenization pipeline reducing vocabulary size by 60% while maintaining semantic coverage"
- "Trained custom word embeddings using Skip-gram architecture, achieving 0.85 correlation with human similarity judgments"
- "Optimized text preprocessing pipeline, improving downstream model accuracy by 12%"
- "Built subword tokenizer handling 15+ languages with single vocabulary"

**Common Pitfalls to Avoid:**

❌ Over-preprocessing: Removing too much information
❌ Ignoring special characters in domain-specific text (code, URLs, hashtags)
❌ Using word-level tokenization for production systems
❌ Training embeddings from scratch when pretrained ones exist
❌ Forgetting to handle `<PAD>`, `<UNK>`, `<START>`, `<END>` tokens
❌ Not normalizing embeddings before computing similarity

**Next Steps (Day 2 Preview):**

- Attention mechanisms: How models focus on relevant tokens
- Positional encodings: Adding sequence order information
- Transformer architecture: Putting it all together
- Transfer learning: Using pretrained models effectively

---

## Quick Reference Card

```
TOKENIZATION DECISION TREE:

Need to handle ANY text? → Use BPE/WordPiece
Building BERT model? → Use WordPiece  
Building GPT model? → Use BPE
Multilingual? → Use BPE (better for rare languages)
Limited compute? → Use smaller vocabulary (15-30K)

EMBEDDING DIMENSIONS:

Small tasks (classification): 50-100 dims
Medium tasks (NER, POS): 100-300 dims  
Large tasks (LM, translation): 300-768 dims
Massive models (GPT, BERT): 768-12,288 dims

PREPROCESSING INTENSITY:

Maximum (search/IR): lowercase, remove stops, stem
Medium (classification): lowercase, remove punctuation
Minimum (generation): keep everything, light cleaning
None (language modeling): raw text only
```

**Tools to Know:**

- **Tokenizers**: HuggingFace `tokenizers`, SentencePiece, tiktoken (OpenAI)
- **Embeddings**: Word2Vec (gensim), GloVe, FastText, BERT embeddings
- **Preprocessing**: NLTK, spaCy, regex
- **Visualization**: t-SNE, UMAP for embedding visualization

---
